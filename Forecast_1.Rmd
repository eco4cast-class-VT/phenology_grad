---
title: "Forecast_Submission"
author: "Garret Dettman, Ben Miller, Whitney Woelmer"
date: "4/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Libraries
```{r}
#remotes::install_github("eco4cast/neon4cast")
#remotes::install_github("nimble-dev/nimble", subdir = "packages/nimble")

library(neon4cast)
library(coda)
library(daymetr)
library(tidyverse)
library(tidybayes)
library(nimble)
library(readr)
library(aws.s3)
library(lubridate)
library(neonUtilities)
library(dplyr)
library(ggplot2)
```

# Define Function for GCC estimation

```{r}
#### function to fit nimble model to historical data at all NEON phenology sites and forecast phenology at all sites using post-predictive function outside of nimble

pheno_forecast <- function(data, ## REQUIRED cols: time, siteID, gcc_90, gcc_sd, doy, daily_min, daily_max
                           forecast_start_date,
                           forecast_end_date,
                           Tbase){ # Tbase needs to be set for clarity at function call

# Calculate driver variable, cumulative GDD, from daily min and max temps
data <- data %>% mutate(GDD_day = ((daily_max + daily_min)/2) - Tbase) %>% 
  mutate(GDD_day = ifelse(GDD_day > 0, GDD_day, 0)) %>% 
  mutate(year = year(time)) %>% 
  group_by(siteID, year) %>% 
  mutate(GDD = cumsum(GDD_day)) %>% 
  filter(doy < 180)

# separate out training and forecast time periods
train <- data[data$time < forecast_start_date, ]
forecast <- data[data$time > forecast_start_date, ] # Defines forecast dataframe from NOAA to be used 
                                                    # later on in function

### DEFINE NIMBLE MODEL
logistic <- nimbleCode({
  ## Priors
  theta1 ~ dnorm(0, sd = 10000)
  theta2 ~ dnorm(0, sd = 10000)
  theta3 <- -50
  theta4 ~ dnorm(0, sd = 10000)
  sd_data ~ dunif(0.00001, 100)
  
  # Loop through data points
  for(i in 1:n){
      ## Process model
      pred[i] <- theta1 + theta2 * exp(theta3 + theta4 * x[i]) / (1 + exp(theta3 + theta4 * x[i])) 
      ## Data model
      y[i]  ~ dnorm(pred[i], sd = sd_data)
  }
})

### Define Constants and Data
constants <- list(n = length(train$gcc_90))
data <- list(x = train$GDD,
             y = train$gcc_90)

### Initialize Chains
nchain <- 3
inits <- list()
for(i in 1:nchain){
  inits[[i]] <- list(theta1 = rnorm(1, 0.34, 0.05), 
                     theta2 = rnorm(1, 0.11, 0.05),
                     theta4 = rnorm(1, 0.4, 0.05),
                     sd_data = runif(1, 0.05, 0.15 ))
}

### Runs NIMBLE model
nimble.out <- nimbleMCMC(code = logistic,
                         data = data,
                         inits = inits,
                         constants = constants,
                         monitors = c("theta1", 
                                      "theta2",
                                      "theta4", 
                                      "sd_data"),
                         niter = 10000,
                         nchains = 3,
                         samplesAsCodaMCMC = TRUE)

### Set Burn Value and burn start of chains
burnin <- 1000                               
nimble.burn <- window(nimble.out, start=burnin)

### Analyitics
traceplot(nimble.burn) 
gelman.diag(nimble.burn)  ## determine convergence

### Sample Chain
chain <- nimble.burn %>%
  tidybayes::spread_draws(theta1, theta2, theta4, sd_data)

pred_function <- function(x, theta1, theta2, theta3, theta4){
  theta1 + theta2 * exp(theta3 + theta4 * x) / (1 + exp(theta3 + theta4 * x))
}
num_samples <- 1000

## Data setup
new <- forecast # Sets up data based on the forecasted needs for graphing purposes
x_new <- forecast$GDD # Defines driver variables
pred_posterior_mean <- matrix(NA, num_samples, length(x_new))   # storage for all simulations, blank matrix
y_posterior <- matrix(NA, num_samples, length(x_new)) # Sets up empty y posterior for each run

### Runs model on sampled chain values for x values that need to be forecast
for(i in 1:num_samples){
  sample_index <- sample(x = 1:nrow(chain), size = 1, replace = TRUE)
  pred_posterior_mean[i, ] <-pred_function(x_new, 
                                           theta1 = chain$theta1[sample_index],
                                           theta2 = chain$theta2[sample_index],
                                           theta3 = -50,
                                           theta4 = chain$theta4[sample_index])
  y_posterior[i, ] <- rnorm(length(x_new), pred_posterior_mean[i, ], sd = chain$sd_data[sample_index])
}

### Calculates confidence in values
conf_int <- apply(y_posterior, 2, quantile, c(0.025, 0.5, 0.975), na.rm = TRUE) # process error
pred_mean <- apply(y_posterior, 2, mean, na.rm = TRUE) # Predicts mean
pred_sd <- apply(y_posterior, 2, sd, na.rm = TRUE) # calculates sd of prediction
obs_conf_int <- apply(pred_posterior_mean, 2, quantile, c(0.025, 0.5, 0.975), na.rm = TRUE) # observation error


out <- tibble(time = new$time,            
              siteID = new$siteID,
              obs_flag = 2, # not sure what this is for currently
              mean = pred_mean,
              sd = pred_sd,
              ## Don't need CI for challenge but it is nice for graph.
              Conf_interv_02.5 = conf_int[1, ], #+ obs_conf_int[1, ], 
              Conf_interv_97.5 = conf_int[3, ], #+ obs_conf_int[3, ],
              forecast = 1,
              data_assimilation = 0,
              x = x_new,
              obs = new$gcc_90,
              doy = new$doy)
  

prediction_plot <- ggplot(out, aes(x = doy, y = mean)) +
  geom_ribbon(aes(ymin = Conf_interv_02.5, ymax = Conf_interv_97.5), fill = "lightblue", alpha = 0.5) +
  geom_line() +
  facet_wrap(~siteID) +
  geom_point(aes(y = obs), color = "gray", alpha = 0.3) +
  labs(y = "Phenology GCC Logistic model")
print(prediction_plot)

# Generates neat output file and writes a .csv file for the output
out.2 <- out %>% 
  pivot_longer(mean:sd, 
               names_to = "statistic",
               values_to = "gcc_90") %>% 
  select(time, siteID, obs_flag, forecast, data_assimilation, statistic, gcc_90)

write.csv(out.2, paste0('phenology-', Sys.Date(), '-VT_Ph_GDD', '.csv'))

#### ------------------------------------END OF FUNCTION---------------------------------------####
}
```

# Load historic data and download neaon ensemble members
```{r}
# Reads in historic data to tune parameters
d <- read.csv('phenology_data.csv') # Historic data, needs to be updated occasionaly form NEON
d$year <- year(d$time)
d <- subset(d, select=-GDD) # Removes GDD from historic data

pheno_sites <- c("HARV", "BART", "SCBI", "STEI", "UKFS", "GRSM", "DELA", "CLBJ")
neon4cast::download_noaa(pheno_sites, interval = "1hr") # Downloads NOAA Data for 35 days into the future from today
noaa_fc <- neon4cast::stack_noaa() # Stacks NOAA Data

# Creates doy columns and year columns from downloaded NOAA Data
noaa_fc_avg <- noaa_fc %>%
  mutate(doy = yday(time),
         year = year(time))

# Creates a summary by site, day of year and year of daily min and max temperature
noaa_fc_avg <- noaa_fc_avg %>%
  group_by(siteID, doy, year) %>%
  summarise(daily_min = min(air_temperature),
            daily_max = max(air_temperature))

# Sets up dataframe to be merged with the historic data from NEON
noaa_fc_avg <- noaa_fc_avg %>%
  mutate(time = as.Date(doy, origin = "2020-12-31"),
         siteID = siteID,
         gcc_90 = NA,
         gcc_sd = NA,
         doy = doy,
         daily_min = daily_min,
         daily_max = daily_max,
         year = year)

# Establishes that the gcc columns previously created as 'NA' are numeric so they can be
# merged with the historic data.
noaa_fc_avg$gcc_90 <- as.numeric(noaa_fc_avg$gcc_90)
noaa_fc_avg$gcc_sd <- as.numeric(noaa_fc_avg$gcc_sd)
# Sets the type of the imported historic data as time so it can merge
d$time <- as.Date(d$time)

## Combines historic data with forecasted data
d_noaa <- rbind(d, noaa_fc_avg)
```


```{r}
# Sets the forecast start date, and the forecast end data from d_noaa
forecast_start_date <- as.Date(max(d_noaa$time)) - 35
forecast_end_date <- as.Date(max(d_noaa$time))

# Calls forecasting function on d_noaa
pheno_forecast(data = d_noaa,
               forecast_start_date = forecast_start_date,
               forecast_end_date = forecast_end_date,
               Tbase = 10)
```
