---
title: "Forecast_Submission"
author: "Garret Dettman, Ben Miller, Whitney Woelmer"
date: "4/14/2021"
output: html_document
---

```{r setup, include=True}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#remotes::install_github("eco4cast/neon4cast")
#remotes::install_github("nimble-dev/nimble", subdir = "packages/nimble")

library(neon4cast)
library(coda)
library(daymetr)
library(tidyverse)
library(tidybayes)
library(nimble)
library(readr)
library(aws.s3)
library(lubridate)
library(neonUtilities)
library(dplyr)
library(ggplot2)
```



```{r}
# function to fit nimble model to historical data at all NEON phenology sites and forecast phenology at all sites using post-predictive function outside of nimble
pheno_forecast <- function(data, ## should have cols: time, siteID, gcc_90, gcc_sd, doy, daily_min, daily_max
                           forecast_start_date,
                           forecast_end_date,
                           Tbase = 10){
  # calculate driver variable, cumulative GDD, from daily min and max temps
data <- data %>% mutate(GDD_day = ((daily_max + daily_min)/2) - Tbase) %>% 
  mutate(GDD_day = ifelse(GDD_day > 0, GDD_day, 0)) %>% 
  mutate(year = year(time)) %>% 
  group_by(siteID, year) %>% 
  mutate(GDD = cumsum(GDD_day)) %>% 
  filter(doy < 180)
# separate out training and forecast time periods
train <- data[data$time < forecast_start_date, ]
forecast <- data[data$time > forecast_start_date, ]
logistic <- nimbleCode({
  # Priors
  theta1 ~ dnorm(0, sd = 10000)
  theta2 ~ dnorm(0, sd = 10000)
  theta3 <- -50
  theta4 ~ dnorm(0, sd = 10000)
  sd_data ~ dunif(0.00001, 100)
  #Loop through data points
  for(i in 1:n){
      # Process model
      pred[i] <- theta1 + theta2 * exp(theta3 + theta4 * x[i]) / (1 + exp(theta3 + theta4 * x[i])) 
      # Data model
      y[i]  ~ dnorm(pred[i], sd = sd_data)
  }
})
constants <- list(n = length(train$gcc_90))
data <- list(x = train$GDD,
             y = train$gcc_90)
nchain <- 3
inits <- list()
for(i in 1:nchain){
  inits[[i]] <- list(theta1 = rnorm(1, 0.34, 0.05), 
                     theta2 = rnorm(1, 0.11, 0.05),
                     #theta3 = rnorm(1, -50, 5),
                     theta4 = rnorm(1, 0.4, 0.05),
                     sd_data = runif(1, 0.05, 0.15 ))
}
nimble.out <- nimbleMCMC(code = logistic,
                           data = data,
                           inits = inits,
                           constants = constants,
                           monitors = c("theta1", 
                                        "theta2",
                                       #"theta3", 
                                        "theta4", 
                                        "sd_data"),
                           niter = 10000,
                           nchains = 3,
                           samplesAsCodaMCMC = TRUE)
#plot(nimble.out) #
#gelman.diag(nimble.out)  ## determine convergence
#traceplot(nimble.out)
burnin <- 1000                               
nimble.burn <- window(nimble.out, start=burnin)
traceplot(nimble.burn) #
#effectiveSize(nimble.burn)
gelman.diag(nimble.burn)  ## determine convergence
chain <- nimble.burn %>%
  tidybayes::spread_draws(theta1, theta2, theta4, sd_data)
pred_function <- function(x, theta1, theta2, theta3, theta4){
  theta1 + theta2 * exp(theta3 + theta4 * x) / (1 + exp(theta3 + theta4 * x))
}
num_samples <- 1000
#new <- d[d$siteID=='HARV',]
new <- forecast
x_new <- forecast$GDD
pred_posterior_mean <- matrix(NA, num_samples, length(x_new))   # storage for all simulations
y_posterior <- matrix(NA, num_samples, length(x_new)) 
for(i in 1:num_samples){
  sample_index <- sample(x = 1:nrow(chain), size = 1, replace = TRUE)
  pred_posterior_mean[i, ] <-pred_function(x_new, 
                                           theta1 = chain$theta1[sample_index],
                                           theta2 = chain$theta2[sample_index],
                                           theta3 = -50,
                                           theta4 = chain$theta4[sample_index])
  y_posterior[i, ] <- rnorm(length(x_new), pred_posterior_mean[i, ], sd = chain$sd_data[sample_index])
  
}
conf_int <- apply(y_posterior, 2, quantile, c(0.025, 0.5, 0.975), na.rm = TRUE) # process error
pred_mean <- apply(y_posterior, 2, mean, na.rm = TRUE)
pred_sd <- apply(y_posterior, 2, sd, na.rm = TRUE)
obs_conf_int <- apply(pred_posterior_mean, 2, quantile, c(0.025, 0.5, 0.975), na.rm = TRUE) # observation error
out <- tibble(time = new$time,            
              siteID = new$siteID,
              obs_flag = 2, # not sure what this is for currently
              mean = pred_mean,
              sd = pred_sd,
              Conf_interv_02.5 = conf_int[1, ], #+ obs_conf_int[1, ],
              Conf_interv_97.5 = conf_int[3, ], #+ obs_conf_int[3, ],
              forecast = 1,
              data_assimilation = 0,
              x = x_new,
              obs = new$gcc_90,
              doy = new$doy)
  
ggplot(out, aes(x = doy, y = mean)) +
  geom_ribbon(aes(ymin = Conf_interv_02.5, ymax = Conf_interv_97.5), fill = "lightblue", alpha = 0.5) +
  geom_line() +
  facet_wrap(~siteID) +
  geom_point(aes(y = obs), color = "gray", alpha = 0.3) +
  labs(y = "Phenology GCC Logistic model")
out.2 <- out %>% 
  pivot_longer(mean:sd, 
               names_to = "statistic",
               values_to = "gcc_90") %>% 
  select(time, siteID, obs_flag, forecast, data_assimilation, statistic, gcc_90)
write.csv(out.2, paste0('phenology-', Sys.Date(), '-VT_Ph_GDD', '.csv'))
  
}
```

```{r}

d <- read.csv('phenology_data.csv')
d$year <- year(d$time)
Tbase <- 10
pheno_sites <- c("HARV", "BART", "SCBI", "STEI", "UKFS", "GRSM", "DELA", "CLBJ")
download_noaa(pheno_sites, interval = "1hr")
noaa_fc <- stack_noaa()
noaa_fc
noaa_fc_avg <- noaa_fc %>%
  mutate(doy = yday(time),
         year = year(time))
noaa_fc_avg <- noaa_fc_avg %>%
  group_by(siteID, doy, year) %>%
  summarise(daily_min = min(air_temperature),
         daily_max = max(air_temperature))
noaa_fc_avg <- noaa_fc_avg %>%
  mutate(time = as.Date(doy, origin = "2020-12-31"),
         siteID = siteID,
         gcc_90 = NA,
         gcc_sd = NA,
         doy = doy,
         daily_min = daily_min,
         daily_max = daily_max,
         GDD = ((daily_max + daily_min)/2) - Tbase,
         year = year)

noaa_fc_avg$gcc_90 <- as.numeric(noaa_fc_avg$gcc_90)
noaa_fc_avg$gcc_sd <- as.numeric(noaa_fc_avg$gcc_sd)
#d_2021 <- d[d$year == 2021,]
#d_2021 <- d_2021[,1:9]
#d_2021$time <- as.Date(d_2021$time)
#double checking obverlap
#max(d_2021$doy)
#min(noaa_fc_avg$doy)
d$time <- as.Date(d$time)
d_noaa <- rbind(d[,1:9], noaa_fc_avg)
d_noaa <- d_noaa %>%
  mutate(GDD = ifelse(GDD > 0, GDD, 0)) %>% 
  group_by(siteID, year) %>% 
  mutate(GDD = cumsum(GDD))
```


```{r}
# test the function by subsetting the existing dataset 
forecast_start_date <- as.Date(max(pheno$time)) - 35
forecast_end_date <- as.Date(max(pheno$time))
pheno_forecast(data = d, 
               forecast_start_date = forecast_start_date,
               forecast_end_date = forecast_end_date)
```
```{r}
neon4cast::create_model_metadata('phenology-2021-04-14-VT_Ph_GDD.csv')
```
